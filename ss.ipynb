{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivo 1: Predicción de Ventas y Rentabilidad Basadas en el Stock Disponible y Otras Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(gamma=\"auto\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_devices = pd.read_csv('/home/insightlab/Documents/MercadoLibre/arquivos/full_devices.csv', encoding='latin1')\n",
    "df_items_titles_test = pd.read_csv('/home/insightlab/Documents/MercadoLibre/arquivos/items_titles_test.csv', encoding='latin1')\n",
    "df_ofertas_relampago = pd.read_csv('/home/insightlab/Documents/MercadoLibre/arquivos/ofertas_relampago.csv', encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metricas de evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    true_positive, true_negative, false_positive, false_negative = calcule_confusion_matrix_values(y_true, y_pred)\n",
    "    accuracy_rate = (true_positive + true_negative) / y_true.shape[0]\n",
    "    return accuracy_rate\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positive, true_negative, false_positive, false_negative = calcule_confusion_matrix_values(y_true, y_pred)\n",
    "    recall_rate = true_positive / (true_positive + false_negative)\n",
    "    return recall_rate\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positive, true_negative, false_positive, false_negative = calcule_confusion_matrix_values(y_true, y_pred)\n",
    "    precision_rate = true_positive / (true_positive + false_positive)\n",
    "    return precision_rate\n",
    "\n",
    "def f1score (y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2*(p*r)/(p+r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-foldK-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_KFold(X, n_splits=3):\n",
    "  n_samples = X.shape[0]\n",
    "  num_larger_folds = n_samples % n_splits\n",
    "  num_default_elements = n_samples // n_splits\n",
    "  all_index = np.arange(0, n_samples)\n",
    "\n",
    "  for i in range(0, n_splits):\n",
    "    if i < num_larger_folds:\n",
    "        initial_index = i*(num_default_elements+1)\n",
    "        final_index = initial_index + (num_default_elements+1)\n",
    "    else:\n",
    "        initial_index = num_larger_folds*(num_default_elements+1) + (i-num_larger_folds) * num_default_elements\n",
    "        final_index = initial_index + num_default_elements\n",
    "\n",
    "    test_index = np.arange(initial_index, final_index)\n",
    "    train_index = np.setdiff1d(all_index, test_index)\n",
    "\n",
    "    yield train_index, test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_KFold(model, X_train, y_train, n_splits=10):\n",
    "  index = 1\n",
    "  df = pd.DataFrame(columns = [\"Fold\", \"Accuracy\", \"Recall\", \"Precision\", \"F1_Score\"])\n",
    "\n",
    "  for train_index, validation_index in my_KFold(X_train, n_splits=n_splits):\n",
    "    model.fit(X_train[train_index], y_train[train_index])\n",
    "    y_validation_predictions = model.predict(X_train[validation_index])\n",
    "    y_validation = y_train[validation_index]\n",
    "    acc_score = accuracy(y_validation, y_validation_predictions)\n",
    "    rec_score = recall(y_validation, y_validation_predictions)\n",
    "    prec_score = precision(y_validation, y_validation_predictions)\n",
    "    f_score = f1score(y_validation, y_validation_predictions)\n",
    "    df = pd.concat([df, pd.DataFrame([[index, acc_score, rec_score, prec_score, f_score]],columns = [\"Fold\", \"Accuracy\", \"Recall\", \"Precision\", \"F1_Score\"] )])\n",
    "    index+=1\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diplay_model_evaluation(y_train, y_train_predictions, y_test, y_test_predictions):\n",
    "  df = pd.DataFrame(columns = [\"Dados\", \"Accuracy\", \"Recall\", \"Precision\", \"F1_Score\"])\n",
    "\n",
    "  acc_score_train = accuracy(y_train, y_train_predictions)\n",
    "  rec_score_train = recall(y_train, y_train_predictions)\n",
    "  prec_score_train = precision(y_train, y_train_predictions)\n",
    "  f_score_train = f1score(y_train, y_train_predictions)\n",
    "\n",
    "  df = pd.concat([df, pd.DataFrame([['Treino', acc_score_train, rec_score_train, prec_score_train, f_score_train]],\\\n",
    "                                    columns = [\"Dados\", \"Accuracy\", \"Recall\", \"Precision\", \"F1_Score\"] )])\n",
    "\n",
    "  acc_score_test = accuracy(y_test, y_test_predictions)\n",
    "  rec_score_test = recall(y_test, y_test_predictions)\n",
    "  prec_score_test = precision(y_test, y_test_predictions)\n",
    "  f_score_test = f1score(y_test, y_test_predictions)\n",
    "\n",
    "  df = pd.concat([df, pd.DataFrame([['Teste', acc_score_test, rec_score_test, prec_score_test, f_score_test]],\\\n",
    "                                    columns = [\"Dados\", \"Accuracy\", \"Recall\", \"Precision\", \"F1_Score\"] )])\n",
    "\n",
    "  display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcule_confusion_matrix_values(y_true, y_pred):\n",
    "  #Ajustes de dimensões\n",
    "  if y_true.ndim == 1:\n",
    "      y_true = y_true.reshape(y_true.shape[0],1)\n",
    "\n",
    "  if y_pred.ndim == 1:\n",
    "      y_pred = y_pred.reshape(y_pred.shape[0],1)\n",
    "\n",
    "  # Funções que vericam se para cada par, temos um true positive,\n",
    "  # true negative, false positive ou false negative\n",
    "  tp_vect_func = np.vectorize(lambda x, y: 1 if x == y and x==1 else 0)\n",
    "  tn_vect_func = np.vectorize(lambda x, y: 1 if x == y and x==0 else 0)\n",
    "  fp_vect_func = np.vectorize(lambda x, y: 1 if x != y and x==0 else 0)\n",
    "  fn_vect_func = np.vectorize(lambda x, y: 1 if x != y and x==1 else 0)\n",
    "\n",
    "  # Contagem dos valores\n",
    "  TP = np.add.reduce(tp_vect_func(y_true, y_pred))[0]\n",
    "  TN = np.add.reduce(tn_vect_func(y_true, y_pred))[0]\n",
    "  FP = np.add.reduce(fp_vect_func(y_true, y_pred))[0]\n",
    "  FN = np.add.reduce(fn_vect_func(y_true, y_pred))[0]\n",
    "\n",
    "  return TP, TN, FP, FN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
